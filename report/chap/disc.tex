\chapter{Discounted payoff}
\label{chap:disc}

Discounted payoff game is yet another game related to Mean payoff (and hence Parity). Note that the payoffs for both Parity and Mean payoff were prefix independent; this is in stark contrast with the payoffs for finite games. Discounted payoff lies between these two - the payoff depends on the whole infinite path, but can be predicted to any desired accuracy by knowing a large enough prefix. This allows for the use of backward induction like technique (\autoref{thm:finiteMinMax}) to show (and compute) the minimax equilibrium.

Discounted payoff was introduced in \cite{shapley_stochastic} in a more general context of stochastic games. The following is a deterministic version of it.

\section{Definition}
Like in the Mean payoff, start with a graph $G$ and edge weights
\[
    w : E \mapsto \Reals
\]
Assume $|w(e)| \leq W$ for every $e \in E$.

The Discounted payoff game with parameter $ 0 < \lambda < 1$ is $\G^\lambda_w=(G,f^\lambda_w)$ with
\[
    f^\lambda_w(v_0 v_1 \ldots ) =  (1-\lambda)\sum_{i=0}^\infty \lambda^i w(v_i,v_{i+1})
\]
Since $w$ is bounded and $|\lambda|<1$, the series converges absolutely.

$\G^\lambda_w$ has an interpretation in terms of a stopping game -- after each round (for instance after the $\nth$ round) a coin (of bias $\lambda$) is tossed. With probability $(1-\lambda)$ the game stops and the payoff is $w(v_{n-1},v_n)$, otherwise with probability $\lambda$ the game proceeds to the next round. Then $f^\lambda_w(v_0v_1 \ldots)$ is the expected payoff for the path.

%TODO: f^\lambda_{w+M} = f^\lambda_w + M, and give the discounted interpretation and $M (1-\lambda)$ is added so that sum of weights is $W$.

$f^\lambda_w$ satisfies the following recursive equation
\begin{equation}
    f^\lambda_w(v_0 v_1 \ldots ) = (1-\lambda) w(v_0, v_1) + \lambda f^\lambda_w(v_1 v_2 \ldots) \label{eqn:disc-recursive}
\end{equation}
\section{Optimal strategies}
Fix $w$ and $\lambda$ for the rest of this section -- their dependency might be suppressed at some places. The following presentation is adopted from the original proof by Shapely in \cite{shapley_stochastic} and can also be found in \cite{zwick_meanpayoff}.

\begin{theorem}
    \label{thm:disc-minimax}
$\G^\lambda_w$ satisfies the minimax equilibrium \eqref{eq:vecMinMax}. Moreover the value vector $\eta \in \Reals^V$ is the unique fixed point of
\begin{align}
    F : \Reals^V &\mapsto \Reals^V \label{eq:aux-disc-func}\\
    (F(x))_u &= \begin{dcases}
    \min_{v \in N(u)} (1-\lambda) w(u,v) + \lambda \eta_v & \text{ if } u \in V_0\\
    \max_{v \in N(u)} (1-\lambda) w(u,v) + \lambda \eta_v & \text{ if } u \in V_1\\
    \end{dcases}\nonumber
\end{align}
and
\begin{equation}
\begin{aligned}
    \sigma_\eta(u) &= \argmin_{ v \in N(u)}\; (1-\lambda) w(u,v) + \lambda x_v \quad \text{ if } u \in V_0\\
    \tau_\eta(u) &= \argmax_{ v \in N(u)}\; (1-\lambda) w(u,v) + \lambda x_v   \quad \text{ if } u \in V_1
\end{aligned} \label{eq:opt-strategies-disc}
\end{equation}
are positional optimal strategies.
\end{theorem}
\begin{proof}
For any $x \in \Reals^V$ and every $n \in \Nat$, consider the $n$  step game $\mc{H}_x^n=(G,h^n_x)$ where
\begin{align*}
    h^n_x : \PF[n] &\mapsto \Reals\\
    h^n_x(v_0v_1 \ldots v_n) &= (1-\lambda)\left(\sum_{i=0}^{n-1} \lambda^i w(v_i,v_{i+1})\right) + \lambda^n x_{v_n}
\end{align*}
For a fixed $x$, $\mc{H}_x^n$ approximates the game $\G^\lambda_w$ for large $n$. Moreover there is a simple recursion to find the value and optimal strategies for $\mc{H}_x^n$.

\begin{description}
    \item[For $n=1$] Consider the game $\mc{H}_x^1=(G,h^1_x)$ with
\begin{align*}
    h^1_x : \PF[1] &\mapsto \Reals\\
    h^1_x (v_0,v_1) = & (1-\lambda) w(v_0,v_1) + \lambda x_{v_1}
\end{align*}
Define
\begin{align*}
    \sigma_x(u) &= \argmin_{ v \in N(u)}\; (1-\lambda) w(u,v) + \lambda x_v \quad \text{ if } u \in V_0\\
    \tau_x(u) &= \argmax_{ v \in N(u)}\; (1-\lambda) w(u,v) + \lambda x_v   \quad \text{ if } u \in V_1
\end{align*}

It is straightforward to check that $F(x)$ is the value vector and $(\sigma_x,\tau_x)$ are the optimal strategies for $\mc{H}_x^1$.

\item[For $n \geq 2$] We have
\[
    h^n_x(v_0v_1 \ldots v_n) = (1-\lambda)w(v_0,v_1) + \lambda h^{n-1}_x(v_1v_2 \ldots v_n)
\]

%TODO: Expand
Let $y$ is the value vector of $\mc{H}_x^{n-1}$ and $(\sigma',\tau')$ be optimal strategies. Then $F(y)$ will be the value vector of $\mc{H}_x^n$. Let $\sigma^*=[\sigma_y,\sigma']$ be the strategy for \Player{0} which plays $\sigma_y$ in the first round and $\sigma'$ after that. Similarly let $\tau^*=[\tau_y,\tau']$ be the corresponding strategy for \Player{1}. Then $(\sigma^*,\tau^*)$ will be the optimal strategies for $\mc{H}^n_x$.\\
To show this, let $v_0 v_1 \ldots v_n$ be a path conforming with $\sigma^*$. Since $v_1 \ldots v_n$ conforms with $\sigma'$
\begin{align*}
    h^{n-1}_x(v_1 v_2 \ldots v_n) &\leq y\\
    \intertext{hence}
    h^n_x(v_0 v_1 \ldots v_n) &= (1-\lambda) w(v_0,v_1) + \lambda h^{n-1}_x(v_1v_2 \ldots v_n)\\
    &\leq (1-\lambda) w(v_0,v_1) + \lambda y \\
    &\leq F(y)_{v_0}
\end{align*}
The last inequality follows as $v_0v_1$ conforms with $\sigma_y$. Similarly if $v_0 v_1\ldots v_n$ conforms with $\tau^*$, $h^n_x(v_0v_1 \ldots v_n) \geq F(y)_{v_0}$. Hence $F(y)$ is the value vector for $\mc{H}^n_x$.
\end{description}

Hence by using induction this shows that $F^n(x)$ is the value vector for $\mc{H}^n_x$. Let $\sigma^*=[\sigma_{F^{n-1}(y)},\ldots \sigma_{F(y)},\sigma_y]$ be the strategy that plays $\sigma_{F^{n-1}(y)}$ in the first round, $\sigma_{F^{n-2}(y)}$ in the second round and so on. Similarly let $\tau^*=[\tau_{F^{n-1}(y)},\ldots \tau_y]$. Then $(\sigma^*,\tau^*)$ are optimal strategies for $\mc{H}^n_x$. Since $\mc{H}^n_x$ approximates $\G^\lambda_w$ as $n \to \infty$, we expect $F^n(x)$ to converge to the value vector for $G^\lambda_w$.

Now we will use this to show that $\G^\lambda_w$ has a minimax equilibrium. Consider the norm $\normi{.}$ on $\Reals^V$ by
\[
    \normi{x} = \max_{v \in V} |x_v|
\]
Then (using $|\max_{i\in I} a_i - \max_{i \in I} b_i| \leq \max_{i \in I} |a_i-b_i|$ and similarly for $\min$)
\[
    \normi{F(x)-F(y)} \leq \lambda \normi{x-y}
\]
Hence $F$ is a contraction mapping with coefficient $\lambda \in (0,1)$. It follows that (see \cite[Chapter~9]{baby_rudin}) for any $x$, $\lim_n F^n(x) = \eta$ where $F(\eta)=\eta$ is the unique fixed point of $F$. Since $F^m(\eta)=\eta$ for any $m$, $\mc{H}^m_\eta$ has value $\eta$ and $\sigma^*=\sigma_\eta, \tau^*=\tau_\eta$ are positional optimal strategies.

Let $v_0v_1 \ldots$ be a path in $G$ which conforms with $\sigma_\eta$ then
\[
    h^n_x(v_0 v_1 \ldots v_n) = (1-\lambda) \left( \sum_{i=0}^{n-1} \lambda^i w(v_i,v_{i+1}) \right) + \lambda^n \eta_{v_n} \leq \eta_{v_0}
\]
for every $n$. Letting $n \to \infty$
\[
    f^\lambda_w(v_0 v_1 \ldots) \leq \eta_{v_0}
\]
Similarly when $v_0 v_1 \ldots$ conforms with $\tau_\eta$
\[
    f^\lambda_w(v_0 v_1 \ldots) \geq \eta_{v_0}
\]
This shows that $\eta$ is the value vector for $\G^\lambda_w$ and $(\sigma_\eta,\tau_\eta)$ are positional optimal strategies.
\end{proof}

Like in the Mean payoff case, positional determinacy gives us constraints on the optimal value.

\begin{corollary}
    \label{cor:disc-value-bounds}
    Suppose $w$ is integer valued. Let $\eta$ be the payoff vector for the game $\G^\lambda_w$ and let $v \in V$.
    Then $\eta_v = p(\lambda)/q(\lambda)$ for polynomials $p,q$  of degree $\leq |V|+1$ with integer coefficients bounded by $4|W|$.
\end{corollary}
\begin{proof}
    Let $\eta$ be the value vector for $\G^\lambda_w$. Since $(\sigma_\eta,\tau_\eta)$ are positional optimal
    \[
        \pi^v_{\sigma_\eta\tau_\eta}=v_0v_1 \ldots v_{r-1} (v_r v_{r+1} \ldots v_k)^\omega
    \] for some $k \leq |V|$ and $f^\lambda_w(\pi^v_{\sigma_\eta\tau_\eta})=\eta_v$. But
    \begin{align*}
        f^\lambda_w(\pi^v_{\sigma_\eta\tau_\eta}) &= (1-\lambda) \sum_{i=0}^\infty \lambda^i w(v_i,v_{i+1})\\
        &= (1-\lambda) \left[ \sum_{i=0}^{r-1} \lambda^i w(v_i,v_{i+1}) + \left( \sum_{i=r}^k \lambda^i w(v_i,v_{i+1}) \right) (1+\lambda^{k-r+1}+\lambda^{2(k-r+1)} \ldots) \right]\\
        &= (1-\lambda) \left[ \sum_{i=0}^{r-1} \lambda^i w(v_i,v_{i+1}) + \left( \sum_{i=r}^k \lambda^i w(v_i,v_{i+1}) \right) \frac{1}{1- \lambda^{k-r+1}} \right]\\
        &= \frac{p(\lambda)}{q(\lambda)}
    \end{align*}
    $p,q$ are polynomial of degree $\leq |V|+1$ (take $q(\lambda)=1-\lambda^{k-r+1}$) and all the coefficients are integers bounded modulus by $4W$. Note that coefficients not only depend on $v,w$ but even on $\lambda$ (via $(\sigma_\eta,\tau_\eta)$).
\end{proof}

\section{Mean payoff to Discounted payoff}

For any bounded sequence $(a_i)_{i \in \Nat}$
\begin{align*}
    (1-\lambda) \sum_{i=0}^\infty a_i \lambda^i  &= (1-\lambda)^2 \frac{1}{(1-\lambda)} \sum_{i=0}^\infty a_i\lambda^i\\
    &= (1-\lambda)^2 (1+\lambda+\lambda^2+\ldots) (\sum_{i=0}^\infty a_i \lambda^i)\\
    &= (1-\lambda)^2 \sum_{k=0}^\infty \left(\sum_{i=0}^k a_i\right) \lambda^k
\end{align*}
Hence we have
\begin{align}
    (1-\lambda)\sum_{i=0}^\infty a_i\lambda^i &= (1-\lambda)^2\sum_{k=0}^\infty \left(\sum_{i=0}^k a_i\right) \lambda^k \label{eq:disc-to-sum}
\end{align}
And a special case (put $a_i = 1$ for each $i$)
\begin{equation}
    (1-\lambda)^2 \sum_{k=0}^\infty (k+1)\lambda^k = 1 \label{eq:series}
\end{equation}

Using \eqref{eq:disc-to-sum} and \eqref{eq:series} one can prove the following.
\begin{theorem}
    Let $u_\lambda=(1-\lambda) \sum_{i=0}^\infty a_i \lambda^i$ and assume $\lim_{n \to \infty} \frac{1}{n+1}\sum_{i=0}^n a_i = \alpha$. Then $\lim_{\lambda \to 1^-} u_\lambda = \alpha$.
\end{theorem}

Hence as $\lambda \to 1^-$ we expect the discounted game $\G^\lambda_w$ to approximate the mean payoff game $\G_w$. Now we will show this.

\begin{theorem}
    \label{thm:mean-to-discounted}
    Consider a graph $(G,w)$ with edge weights bounded (in modulus) by $W > 0$. Denote by $\G_w$ the Mean payoff game on this graph and by $\G^\lambda_w$ the discounted game with parameter $\lambda$. Let $\eta$ be the value vector for $\G_w$ and $\eta_\lambda$ be the value vector for $\G^\lambda_w$. Then
    \[
        \normi{\eta - \eta_{\lambda}} \leq 2|V|W (1-\lambda)
    \]
\end{theorem}
\begin{proof} From \eqref{eq:disc-to-sum}
\begin{align*}
    f^\lambda_w(v_0 v_1 \ldots) &= (1-\lambda) \sum_{i=0}^\infty \lambda^i w(v_i,v_{i+1})\\
    &= (1-\lambda)^2 \sum_{k=0}^\infty \left( \sum_{i=0}^k w(v_i,v_{i+1}) \right) \lambda^k \numberthis \label{eq:disc-payoff-as-sum}
\end{align*}
Let $(\sigma^*,\tau^*)$ be the stack based optimal strategies for $\G_w$ and let $(\sigma_\lambda,\tau_\lambda)$ be the optimal strategies for $\G^\lambda_w$.

    Let $\pi=v_0v_1 \ldots$ be an infinite path in $G$ that conforms with $\sigma^*$. Then using the same argument as used in the proof of \autoref{thm:meanEquilibrium} we have from \eqref{eq:sum-upperbound}
    \begin{align*}
        \sum_{i=0}^k w(v_i,v_{i+1}) \leq (k+1) \eta_{v_0} + 2|V|W
    \end{align*}
    Combining this with \eqref{eq:disc-payoff-as-sum}
    \begin{align*}
        f^\lambda_w(\pi) &\leq (1-\lambda)^2 \sum_{k=0}^\infty \left( (k+1)\eta_{v_0} + 2|V|W \right) \lambda^k \\
        &= \eta_{v_0}(1-\lambda)^2 \sum_{k=0}^\infty (k+1) \lambda^k + 2|V|W(1-\lambda)^2(\sum_{k=0}^\infty \lambda^k)\\
        &= \eta_{v_0} + 2|V|W(1-\lambda) \tag{Using \eqref{eq:series} and $\sum_{i=0}^\infty \lambda^i = \frac{1}{1-\lambda}$}
    \end{align*}
    Since $\pi$ was any path that conformed with $\sigma^*$, consider $\pi^v_{\sigma^*\tau_\lambda}$. This shows
    \begin{equation}
        \label{eq:diffvalue1}
        (\eta_\lambda)_v \leq f^\lambda_w(\pi^v_{\sigma^*\tau_\lambda}) \leq \eta_v + 2|V|W(1-\lambda)
    \end{equation}
    for every $v \in V$.

    Similarly if $\pi=v_0v_1 \ldots$ is an infinite path in $G$ that conforms with $\tau^*$, from \eqref{eq:sum-lowerbound}
    \begin{align*}
        \sum_{i=0}^k w(v_i,v_{i+1}) \geq (k+1) \eta_{v_0} - 2|V|W
    \end{align*}
    Similar to the above, this combined with \eqref{eq:disc-payoff-as-sum} for $\pi^v_{\sigma_\lambda\tau^*}$ will give
    \begin{equation}
        \label{eq:diffvalue2}
        (\eta_\lambda)_v \geq f^\lambda_w(\pi^v_{\sigma_\lambda\tau^*}) \geq \eta_v - 2|V|W (1-\lambda)
    \end{equation}

    \eqref{eq:diffvalue1} and \eqref{eq:diffvalue2} together show that
    \[
        \normi{\eta - \eta_\lambda} \leq 2|V|W(1-\lambda)
    \]
\end{proof}


Consider the following decision problem for Discounted payoff games
\begin{decision}[DISC]
    \label{dec:disc}
    Given a graph $G$, vertex $v$, edge weights $w$, a discount $\lambda$ and threshold a $t$. Determine whether $\val(\G^\lambda_w,v) \geq t$\footnote{We can assume $t=0$ by considering $w'(e)=w(e)-t$ instead of $w$} or not.
\end{decision}

\begin{corollary}
    \label{cor:mean2disc}
    Given $(G,v)$ with an integer valued $w$, \nameref{dec:meanpayoff} can be reduced to an instance of \nameref{dec:disc} in polynomial time.
\end{corollary}
\begin{proof}
    Let $W$ be the maximum modulus among all the edge-weights in $w$. Set $\lambda=1 -\frac{1}{8|V|^2W}$ and $t=\frac{3}{4|V|}$. For this choice of $\lambda$ by \autoref{thm:mean-to-discounted}, $|\val(\G_w, v) - \val(\G^\lambda_w,v)| \leq \frac{1}{4|V|}$. Combine this with \autoref{cor:valMeanPayoff} to obtain
    \[
        \val(\G_w,v) > 0 \implies \val(\G_w,v) \geq \frac{1}{|V|} \implies \val(\G^\lambda_w,v) \geq \frac{3}{4|V|}
    \]
    and
    \[
        \val(\G^\lambda_w,v) \geq \frac{3}{4|V|} \implies \val(\G_w,v) \geq \frac{1}{2|V|} > 0
    \]
    this shows that $\val(\G_w,v) > 0 \iff \val(\G^\lambda_w) \geq t$. Computing $W$, $\lambda$ and $t$ can be done time polynomial in the input size -- hence this is a polynomial time reduction.
\end{proof}

\section{Complexity}

By \autoref{thm:disc-minimax}, once the unique fixed point $\eta=F(\eta)$ is found, finding the optimal strategies $(\sigma_\eta,\tau_\eta)$ is easy. Hence to solve $\G^\lambda_w$ one  has to find the fixed point of $F$.

By \autoref{cor:disc-value-bounds}, the value vector has a polynomial representation in the input size. Hence one case guess a possible value $\eta'$ (a polynomial size certificate) for the value vector and verify that $F(\eta')=\eta'$. Since $F$ has a unique fixed point there will be a unique guess which works. Hence this shows that

\begin{theorem}
    \label{thm:disc-in-up}
    \nameref{dec:disc} is in $\UP \cap \coUP$.
\end{theorem}
Here $\UP$ is the class of problems accepted by a polynomial time unambiguous turing Machine. An unambiguous turing machine is a nondererministic turing machine which has at most one accepting run. Hence $P \subseteq \UP \subseteq \NP$.

\section{Summary}
We have defined Discounted Payoff games and shown that they have postional optimal strategies (\autoref{thm:disc-minimax}). The value vector is a fixed point an operator $F$ and can be computed by taking repreated iterates. Next we show how the Discounted payoff value vector approximates the value vector for the corresponding Mean payoff game (\autoref{thm:mean-to-discounted}). This is used to give a polynomial time reduction from the decision problem for Mean payoff \nameref{dec:meanpayoff} to that for Discounted payoff \nameref{dec:disc} (\autoref{cor:mean2disc}). Finally we show that \nameref{dec:disc} is in $\UP \cap \coUP$ (\autoref{thm:disc-in-up}).
