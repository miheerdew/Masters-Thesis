\chapter{Discounted payoff}

Discounted payoff game is yet another game related to Mean payoff (and hence Parity). Note that the payoffs for both Parity and Mean payoff were prefix independent; this is in stark contrast with the payoffs for finite games. Discounted payoff lies between these two - the payoff depends on the whole infinite path, but can be predicted to any desired accuracy by knowing a large enough prefix. This allows for the use of backward induction like technique (\autoref{thm:finiteMinMax}) to show (and compute) the minimax equilibrium.

Discounted payoff was introduced in \cite{shapley_stochastic} in a more general context of stochastic games. The following is a deterministic version of it.

\section{Definition}
Like in the Mean payoff, start with a graph $G$ and edge weights
\[
    w : E \mapsto \Reals
\]
Assume $|w(e)| \leq W$ for every $e \in E$.

The Discounted payoff game with parameter $ 0 < \lambda < 1$ is $\G^\lambda_w=(G,f^\lambda_w)$ with
\[
    f^\lambda_w(v_0 v_1 \ldots ) =  (1-\lambda)\sum_{i=0}^\infty \lambda^i w(v_i,v_{i+1})
\]
Since $w$ is bounded and $|\lambda|<1$, the series converges absolutely.

$\G^\lambda_w$ has an interpretation in terms of a stopping game -- after each round (for instance after the $\nth$ round) a coin (of bias $\lambda$) is tossed. With probability $(1-\lambda)$ the game stops and the payoff is $w(v_{n-1},v_n)$, otherwise with probability $\lambda$ the game proceeds to the next round. Then $f^\lambda_w(v_0v_1 \ldots)$ is the expected payoff for the path.

$f^\lambda_w$ satisfies the following recursive equation
\begin{equation}
    f^\lambda_w(v_0 v_1 \ldots ) = (1-\lambda) w(v_0, v_1) + \lambda f^\lambda_w(v_1 v_2 \ldots) \label{eqn:disc-recursive}
\end{equation}
\section{Optimal strategies}
Now we will show that $\G^\lambda_w$ satisfies the minimax equilibrium \eqref{eq:vecMinMax} with value vector $\eta \in \Reals^V$ and optimal positional strategies $(\sigma^*, \tau^*)$. Fix $w$ and $\lambda$ for the rest of this section -- their dependency might be suppressed at some places.

For any $x \in \Reals^V$ and every $n \in \Nat$, consider the $n$  step game $\mc{H}_x^n=(G,h^n_x)$ where
\begin{align*}
    h^n_x : \PF[n] &\mapsto \Reals\\
    h^n_x(v_0v_1 \ldots v_n) &= (1-\lambda)\left(\sum_{i=0}^{n-1} \lambda^i w(v_i,v_{i+1})\right) + \lambda^n x_{v_n} \label{eqn:fn}
\end{align*}
For a fixed $x$, $\mc{H}_x^n$ approximates the game $\G^\lambda_w$ for large $n$. Moreover there is a simple recursion to find the value and optimal strategies for $\mc{H}_x^n$.

\begin{description}
    \item[For $n=1$] Consider the game $\mc{H}_x^1=(G,h^1_x)$ with
\begin{align*}
    h^1_x : \PF[1] &\mapsto \Reals\\
    h^1_x (v_0,v_1) = & (1-\lambda) w(v_0,v_1) + \lambda x_{v_1}
\end{align*}
Define
\begin{align*}
    F : \Reals^V &\mapsto \Reals^V \\
    (F(x))_u &= \begin{dcases}
    \min_{v \in N(u)} (1-\lambda) w(u,v) + \lambda x_v & \text{ if } u \in V_0\\
    \max_{v \in N(u)} (1-\lambda) w(u,v) + \lambda x_v & \text{ if } u \in V_1\\
    \end{dcases}
    \intertext{and}
    \sigma_x(u) &= \argmin_{ v \in N(u)}\; (1-\lambda) w(u,v) + \lambda x_v \quad \text{ if } u \in V_0\\
    \tau_x(u) &= \argmax_{ v \in N(u)}\; (1-\lambda) w(u,v) + \lambda x_v   \quad \text{ if } u \in V_1
\end{align*}
It is straightforward to check that $F(x)$ is the value vector and $(\sigma_x,\tau_x)$ are the optimal strategies for $\mc{H}_x^1$.

\item[For $n \geq 2$] We have
\[
    h^n_x(v_0v_1 \ldots v_n) = (1-\lambda)w(v_0,v_1) + \lambda h^{n-1}_x(v_1v_2 \ldots v_n)
\]

%TODO: Expand
Let $y$ is the value vector of $\mc{H}_x^{n-1}$ and $(\sigma',\tau')$ be optimal strategies. Then $F(y)$ will be the value vector of $\mc{H}_x^n$. Let $\sigma^*=[\sigma_y,\sigma']$ be the strategy for \Player{0} which plays $\sigma_y$ in the first round and $\sigma'$ after that. Similarly let $\tau^*=[\tau_y,\tau']$ be the corresponding strategy for \Player{1}. Then $(\sigma^*,\tau^*)$ will be the optimal strategies for $\mc{H}^n_x$.\\
To show this, let $v_0 v_1 \ldots v_n$ be a path conforming with $\sigma^*$. Since $v_1 \ldots v_n$ conforms with $\sigma'$
\begin{align*}
    h^{n-1}_x(v_1 v_2 \ldots v_n) &\leq y\\
    \intertext{hence}
    h^n_x(v_0 v_1 \ldots v_n) &= (1-\lambda) w(v_0,v_1) + \lambda h^{n-1}_x(v_1v_2 \ldots v_n)\\
    &\leq (1-\lambda) w(v_0,v_1) + \lambda y \\
    &\leq F(y)_{v_0}
\end{align*}
The last inequality follows as $v_0v_1$ conforms with $\sigma_y$. Similarly if $v_0 v_1\ldots v_n$ conforms with $\tau^*$, $h^n_x(v_0v_1 \ldots v_n) \geq F(y)_{v_0}$. Hence $F(y)$ is the value vector for $\mc{H}^n_x$.
\end{description}

Hence by using induction this shows that $F^n(x)$ is the value vector for $\mc{H}^n_x$, and $\sigma^*=[\sigma_{F^n(y)},\ldots \sigma_{F(y)},\sigma_y]$ and $\tau^*=[\tau_{F^n(y)},\ldots \tau_{F(y)},\tau_y]$ are the optimal strategies. Since $\mc{H}^n_x$ approximates $\G^\lambda_w$ as $n \to \infty$, we expect $F^n(x)$ to converge to the value vector for $G^\lambda_w$.

Now we will use this to show that $\G^\lambda_w$ has a minimax equilibrium. Consider the norm $\normi{.}$ on $\Reals^V$ by
\[
    \normi{x} = \max_{v \in V} |x_v|
\]
Then (using $|\max_{i\in I} a_i - \max_{i \in I} b_i| \leq \max_{i \in I} |a_i-b_i|$ and similarly for $\min$)
\[
    \normi{F(x)-F(y)} \leq \lambda \normi{x-y}
\]
Hence $F$ is a contraction mapping with coefficient $\lambda \in (0,1)$. Hence it follows that for any $x$, $\lim_n F^n(x) = \eta$ where $F(\eta)=\eta$ is the unique fixed point of $F$. Since $F^m(\eta)=\eta$ for any $m$, $\mc{H}^m_\eta$ has value $\eta$ and $\sigma^*=\sigma_\eta, \tau^*=\tau_\eta$ are positional optimal strategies.

Let $v_0v_1 \ldots$ be a path which conforms with $\sigma_\eta$ then
\[
    h^n_x(v_0 v_1 \ldots v_n) = (1-\lambda) \left( \sum_{i=0}^{n-1} \lambda^i w(v_i,v_{i+1}) \right) + \lambda^n \eta_{v_n} \leq \eta_{v_0}
\]
for every $n$. Letting $n \to \infty$
\[
    f^\lambda_w(v_0 v_1 \ldots) \leq \eta_{v_0}
\]
Similarly when $v_0 v_1 \ldots$ comforms with $\tau_\eta$
\[
    f^\lambda_w(v_0 v_1 \ldots) \geq \eta_{v_0}
\]
This shows that $\eta$ is the value vector for $\G^\lambda_w$ and $(\sigma_\eta,\tau_\eta)$ are positional optimal strategies.

\section{Mean payoff to Discounted payoff}

For any bounded sequence $(a_i)_{i \in \Nat}$
\begin{align*}
    (1-\lambda) \sum_{i=0}^\infty a_i \lambda^i  &= (1-\lambda)^2 \frac{1}{(1-\lambda)} \sum_{i=0}^\infty a_i\lambda^i\\
    &= (1-\lambda)^2 (1+\lambda+\lambda^2+\ldots) (\sum_{i=0}^\infty a_i \lambda^i)\\
    &= (1-\lambda)^2 \sum_{k=0}^\infty \left(\sum_{i=0}^k a_i\right) \lambda^k
\end{align*}
Hence we have
\begin{align}
    (1-\lambda)\sum_{i=0}^\infty a_i\lambda^i &= (1-\lambda)^2\sum_{k=0}^\infty \left(\sum_{i=0}^k a_i\right) \lambda^k \label{eq:disc-to-sum}
\end{align}
And a special case (put $a_i = 1$ for each $i$)
\begin{equation}
    (1-\lambda)^2 \sum_{k=0}^\infty (k+1)\lambda^k = 1 \label{eq:series}
\end{equation}

Using \eqref{eq:disc-to-sum} and \eqref{eq:series} one can prove the following.
\begin{theorem}
    Let $u_\lambda=(1-\lambda) \sum_{i=0}^\infty a_i \lambda^i$ and assume $\lim_{n \to \infty} \frac{1}{n+1}\sum_{i=0}^n a_i = \alpha$. Then $\lim_{\lambda \to 1^-} u_\lambda = \alpha$.
\end{theorem}

Hence as $\lambda \to 1^-$ we expect the discounted game $\G^\lambda_w$ to approximate the mean payoff game $\G_w$. Now we will show this.

\begin{theorem}
    \label{thm:mean-to-discounted}
    Consider a graph $(G,w)$ with edge weights bounded (in modulus) by $W > 0$. Denote by $\G_w$ the Mean payoff game on this graph and by $\G^\lambda_w$ the discounted game with parameter $\lambda$. Let $\eta$ be the value vector for $\G_w$ and $\eta_\lambda$ be the value vector for $\G^\lambda_w$. Then
    \[
        \normi{\eta - \eta_{\lambda}} \leq 2|V|W (1-\lambda)
    \]

\end{theorem}
%Using this
%Hence if $\frac{\sum_{i=0}^n a_i}{n+1} \to \alpha$ -- i.e. $\sum_{i=0}^n a_i = (n+1)(\alpha + d_n)$ with $\lim d_n = 0$ then
%\begin{align*}
    %u_\lambda((a_i)) &= (1-\lambda)^2 \sum_{k=0}^\infty (k+1)(\alpha + d_k) \lambda^k\\
    %&= \alpha + (1-\lambda)^2 \sum_{k=0}^\infty (k+1) d_k \lambda^k \qquad \text{ using \eqref{eqn:series} }\\
    %&= \alpha + \delta_\lambda((d_k))
%\end{align*}
%Pick an $\epsilon > 0$. Observe that if $\forall n \; |d_n| \leq \epsilon$ then because of \eqref{eqn:series}  $|\delta_\lambda((d_k))| \leq \epsilon$.\\
%Since we have $\lim d_n = 0$ we have a $N$ so that $\forall n\geq N \; |d_n| \leq \frac{\epsilon}{2}$. We can choose a $\lambda_0$ close enough to $1$ so that $\forall \lambda \geq \lambda_0$ sum of the first $N$ terms of $\delta_\lambda((d_k))$ is  $\leq \frac{\epsilon}{2}$. And hence $\forall \lambda \geq \lambda_0$
%\[
    %|u_\lambda((a_i)) - \alpha| = |\delta_\lambda((d_k))| \leq \frac{\epsilon}{2} + \frac{\epsilon}{2} \leq \epsilon
%\]

Hence if $\lim_n \frac{1}{n+1} \sum_{i=0}^n a_i = \alpha$, then $\alpha = \lim_{\lambda \to 1^-} (1-\lambda) \sum_{i=0}^\infty a_i \lambda^i$. This shows why we expect $\G_\lambda$ to approximate the mean payoff game $\G_m$ as $\lambda \to 1$.

Now we will show that the values of $\G_\lambda$ will converge to the value for the mean payoff game $\G_m$. Let $\eta_\lambda$ be the values for $\G_\lambda$, and let $\eta$ be for $\G_m$. Consider the stack based optimal strategy $\sigma_m$ in $\G_m$ for player 0. How will this strategy fare in $\G_\lambda$? Let $\tau^\lambda$ be the optimal strategy for player 1 in $\G_\lambda$. Let $\pi^{v_0}_{\sigma_m\tau_\lambda}=v_0v_1 \ldots$, then
\begin{align}
    f_\lambda(\pi^{v_0}_{\sigma_m\tau_\lambda}) &= u_\lambda((w(v_i,v_{i+1}))\\
    &= (1-\lambda)^2 \sum_{k=0}^\infty (\sum_{i=0}^k w(v_i,v_{i+1})) \lambda^k \label{eq:sumdisc}
\end{align}
Now suppose that at the $nth$ stage there are $r(n)$ edges on the stack. Then
\begin{align}
    \sum_{i=0}^{n-1} w(v_i,v_{i+1}) &\leq (n-r(n)) \eta_{v_0} + M r(n)\\
    &\leq n \eta_{v_0} + r(n)(M-\eta_{v_0})\\
    &\leq n \eta_{v_0} + 2|V|M
\end{align}
Hence
\begin{align}
    f_\lambda(\pi^{v_0}_{\sigma_m\tau_\lambda}) &\leq (1-\lambda)^2 \sum_{k=0}^\infty ((k+1)\eta_{v_0} + 2|V|M)\lambda^k\\
    &\leq \eta_{v_0} + 2|V|M (1-\lambda)
\end{align}
But since $\tau_\lambda$ is optimal for player 1 in $\G_\lambda$, we have $f_\lambda(\pi^{v_0}_{\sigma_m\tau_\lambda}) \geq (\eta_\lambda)_{v_0}$. Hence -
\begin{align}
    (\eta_\lambda)_{v_0} \leq \eta_{v_0} + 2|V|M (1-\lambda) \label{ineq:close1}
\end{align}

Similarly start with an optimal strategy $\tau^m$ for player 1 in $\G_m$ and play it against the optimal strategy $\sigma_\lambda$ for player 0 in $\G_\lambda$. Then the resulting path $\pi^{v_0}_{\sigma_\lambda\tau^*}=v_0v_1\ldots$ will satisfy (similar to \eqref{eq:sumdisc}) -
\[
    f_\lambda(\pi^{v_0}_{\sigma_\lambda\tau_m}) = (1-\lambda)^2 \sum_{k=0}^\infty (\sum_{i=0}^k w(v_i,v_{i+1})) \lambda^k  \\
\]
And using the stack argument,
\begin{align}
    \sum_{i=0}^{n-1} w(v_i,v_{i+1}) &\geq (n-r(n)) \eta_{v_0} - M r(n)\\
    &\geq n \eta_{v_0} - r(n)(M+\eta_{v_0})\\
    &\geq n \eta_{v_0} - 2|V|M
\end{align}
Hence
\begin{equation}
    (\eta_\lambda)_{v_0} \geq f_\lambda(\pi^{v_0}_{\sigma_\lambda\tau_m}) \geq \eta_{v_0} - 2|V|M (1-\lambda) \label{ineq:close2}
\end{equation}

Now using \eqref{ineq:close1} and \eqref{ineq:close2} from every $v_0 \in V$,
\begin{equation}
    \normi{\eta_\lambda - \eta_m} \leq 2|V|M(1-\lambda)
\end{equation}
